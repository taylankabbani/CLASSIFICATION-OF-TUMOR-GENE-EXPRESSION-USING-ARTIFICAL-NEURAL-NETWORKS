---
title: "second-run-with-all-cancer"
author: "Taylan Kabbani"
date: "March 27, 2018"
output: html_document
---

```{r}
library(tidyverse)
library(keras)

all_cancer_labeled <- readRDS("all_cancer_labeled.rds")
```


# Preparing input data

Refer to `all_cancer_types.Rmd` file for how B.Darendeli selected 784 genes (saved as `all_cancer_types.rds`). 

```{r}
no_of_samples <- ncol(all_cancer_labeled) - 1

set.seed(13)
v <- all_cancer_labeled %>% 
  gather(sample, value, -gene) %>% 
  mutate(value =ifelse(value < 1, 1, value)) %>% 
  #separate(sample, c("patient", "case", "stage"), sep="_") %>% 
  mutate(log_value = log(value)/16.03866) %>% 
  select(-value) %>% 
  spread(sample, log_value) %>% 
  sample_n(784) %>% 
  select(-gene) %>%
  as.matrix(byrow = TRUE) %>% 
  { colnames(.) ->> sample_names      # ugly hack to save intermediate https://stackoverflow.com/a/47879695/4579196
    dim(.) <- c(28,28,no_of_samples)  # changing dimensions of matrix and printing again
    .                                 # so that pipe is not broken: https://stackoverflow.com/a/44052271/4579196
  } %>% 
  aperm(c(3,1,2))

```

Check size of the final 3d array. Its dimensions should be no_of_samples, 28 , 28

```{r}
dim(v)  # should be no_of_samples, 28 , 28
# d[1,1:28,1:28] should print data for first patient
```

Finally, use `array_reshape()` from `keras` package to reshape the data to be used as input layer.

```{r}
input <- array_reshape(v, c(no_of_samples, 28, 28, 1))
```

# Preparing labels array

`sample_names` object kept from previous step is used to convert sample names into 0 or 1. If sample name contains "normal" then it's assigned 0, other labels, such as "Tumor" or "Metastatic" are assigned 1.

A 1d array, `labels` , holds the labels. And the distribution of labels is;

```{r}
labels <- sample_names %>% 
  as_data_frame() %>% 
  filter(value !="gene") %>% 
  # seperate by underscore and get last column
  separate(value, c("TCGA_id","patient", "case", "stage","label"), sep="_") %T>%  # tee-operator
  { count(., label) %>% print() }  %>%   # count values - an ugly hack
  pull(label) %>% 
  array()
```

Check size of array. Should be 1d array as long as `no_of_samples`

```{r}
# class(labels)
dim(labels)
```


# Verify data

Let's check the data.

```{r}
library(ggplot2)

dim(v)
# this needs to be tidied
image <- v[1,1:28,1:28]
long <- as.data.frame.table(image)
long[1:2] <- lapply(long[1:2], as.numeric) 

ggplot(data = long, aes(x=Var2, y=Var1, fill=Freq)) + 
 geom_tile() + 
 scale_y_reverse() +
 coord_fixed(ratio=1) +
 scale_fill_gradient(low = "white",high = "black", limits=c(0,1)) 
```

# Starting to run Keras

The code from [Rstudio Keras MNIST CNN example](https://keras.rstudio.com/articles/examples/mnist_cnn.html) is modified for our own sample

```{r}
batch_size <- 128
num_classes <- 6
epochs <- 20

img_rows <- 28
img_cols <- 28

# The data, shuffled and split between train and test sets
x_train <- input
y_train <- labels
#x_test <- mnist$test$x
#y_test <- mnist$test$y

# Redefine  dimension of train/test inputs

#we already did the arrat_reshape part above
#x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# Transform RGB values into [0,1] range
#x_train <- x_train / 255
#x_test <- x_test / 255

cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
#cat(nrow(x_test), 'test samples\n')

# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
#y_test <- to_categorical(y_test, num_classes)

# Define Model -----------------------------------------------------------

# Define model
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = input_shape) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = num_classes, activation = 'softmax')

# Compile model
model %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)
```


```{r}
# Train model
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2
)
```



